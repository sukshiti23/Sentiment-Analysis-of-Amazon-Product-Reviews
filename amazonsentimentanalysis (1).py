# -*- coding: utf-8 -*-
"""AmazonSentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s-VBX-ktEBxo3qnOpy9RHW0HtpPDa_B3

#AMAZON PRODUCT REVIEWS SENTIMENT ANALYSIS

Sentiment Analysis is a Natural Language Processing(NLP) technique of analyzing large volumes of text to determine whether it expresses a positive sentiment, a negative sentiment or a neutral sentiment.

 Sentiment analysis systems help companies better understand their customers, deliver stronger customer experiences and improve their brand reputation.

The aim of this project is to predict whether the reviews given for Amazon products are positive or negative

**Importing Libraries and Dataset**
"""

import warnings
warnings.filterwarnings('ignore') #hides warning messages

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score

from transformers import pipeline #we can use pretrained transformer models

import torch #uses pytorch tensors

#roc_auc score -> tells how well the model can distinguish between the positive and negative

classifier = pipeline('sentiment-analysis', truncation=True) #sentiment analysis model used
type(classifier)

data = pd.read_csv('/content/Amazon-Product-Reviews-Sentiment-Analysis-in-Python-Dataset (1).csv')
data

"""**Data Preprocessing and Cleaning**"""

data.head()

data.tail()

data.info()

"""The sentiments in the dataset are based on the star-rating by Amazon. We need to convert the ratings into Positive (1) or Negative (0). So we change the Sentiment value as 0 if stars <=3 and 1 if stars > 3"""

data.loc[data['Sentiment']<= 3, 'Sentiment'] = 0

data.loc[data['Sentiment']>3, 'Sentiment'] = 1

data

data.tail()

data.isnull().sum()

# Drop rows where 'Review' is NaN
data = data[data['Review'].notnull()]
#drop rows where 'Review' is empty or just spaces
data = data[data['Review'].astype(str).str.strip() != ""]
# Reset index
data.reset_index(drop=True, inplace=True)

data['Sentiment'].value_counts()

data.isnull().sum()

"""**CountPlot for displaying distribution of Positive and Negative Sentiments**"""

sns.countplot(data, x='Sentiment', palette='viridis')
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.title("Sentiment Count")
plt.show()

"""As seen from the graph, majority of the reviews are below 3 stars.

Using NLTK library for NLP
"""

import warnings
warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import TfidfVectorizer #to convert text into numerical values
from wordcloud import WordCloud

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

data['Sentiment'].value_counts()

"""Word Cloud to check the words present in Negative Reviews"""

consolidated = ''.join(word for word in data['Review'][data['Sentiment']==0].astype(str))
wordCloud=WordCloud(width=1600,height=800,random_state=21,max_font_size=110)
plt.figure(figsize=(15,10))
plt.imshow(wordCloud.generate(consolidated),interpolation='bilinear')
plt.axis('off')
plt.show()

"""Word Cloud to check the words present in Positive Reviews"""

consolidated = ''.join(word for word in data['Review'][data['Sentiment']==1].astype(str))
wordCloud=WordCloud(width=1600,height=800,random_state=21,max_font_size=110)
plt.figure(figsize=(15,10))
plt.imshow(wordCloud.generate(consolidated),interpolation='bilinear')
plt.axis('off')
plt.show()

"""From the word cloud for negative reviews we can find out the top complaint words and thus, the issue with the products or services can be addressed"""

from sklearn.feature_extraction.text import TfidfVectorizer

negative_review = data['Review'][data['Sentiment']==0]

vectorizer = TfidfVectorizer(stop_words='english', max_features=10)
x = vectorizer.fit_transform(negative_review)

top_negative_words = vectorizer.get_feature_names_out()
print("Top complaint keywords:", top_negative_words)

"""**Tfidf** -> It's a numerical statistic used in Natural Language Processing (NLP) to evaluate how important a word is in a document relative to a collection (corpus) of documents.

#Model Training, Evaluation & Prediction
"""

#converting text into vectors
cv = TfidfVectorizer(max_features=2500)
X = cv.fit_transform(data['Review']).toarray()
#Learns the vocabulary from the Review column and converts each review into a vector of TF-IDF scores

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, data['Sentiment'], test_size=0.2, random_state=0) #taking 20% of data for training

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression()
model.fit(x_train, y_train)
pred = model.predict(x_test)
print("Accuracy Score is : ", accuracy_score(y_test,pred)) #compares the correct values from dataset and the mode predicted value

"""We got the accuracy score 82% which tells us that our model predicts the correct outcome for the 82% of the data.

Confusion Matrix
"""

from sklearn import metrics
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, pred)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,
                                            display_labels = [False, True])
cm_display.plot()
plt.show()

